#ä¸€æ¬¡åŒæ™‚çœ‹ä¸€å€‹ç‰‡æ®µä¸¦ä¸”çµ¦å‡ºè¦ºå¾—æ‡‰è©²åˆ†å‰²çš„é»
import os
import re
import argparse
import time
from dotenv import load_dotenv
from tqdm import tqdm
from zhconv_rs import zhconv
import ollama
from config import BACK_END_MODEL, AI_MODEL, OLLAMA_URL
from openai import OpenAI

# ========= INIT =========
starttime = time.time()
parser = argparse.ArgumentParser(description="ç”¨ GPT å°‡é€å­—ç¨¿ä¾ä¸»é¡Œåˆ†æ®µä¸¦è¼¸å‡º md æª”")
parser.add_argument("input_file", help="è¼¸å…¥é€å­—ç¨¿æª”æ¡ˆï¼ˆæ¯è¡Œæ ¼å¼ç‚º speakerX: èªªè©±å…§å®¹ï¼‰")
args = parser.parse_args()
input_file = args.input_file
MAX_TEXT_LEN = 3000
output_dir = os.path.splitext(os.path.basename(input_file))[0] + "_topics"
os.makedirs(output_dir, exist_ok=True)

# ========= AI åˆå§‹åŒ– =========
load_dotenv()
openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

def ai_response(conversation_history, max_tokens=1000):
    if BACK_END_MODEL == 'openai':
        response = openai_client.chat.completions.create(
            model=AI_MODEL,
            messages=conversation_history
        )
        assistant_reply = response.choices[0].message.content
    elif BACK_END_MODEL == 'ollama':
        response = ollama.Client(host=OLLAMA_URL).chat(
            model=AI_MODEL,
            messages=conversation_history
        )
        assistant_reply = response['message']['content'].strip()
        if AI_MODEL.startswith("deepseek"):
            assistant_reply = re.sub(r'<think>(.*)</think>', '', assistant_reply, flags=re.DOTALL).strip()
    assistant_reply = zhconv(assistant_reply, "zh-tw")
    return assistant_reply

# ========= è®€å…¥é€å­—ç¨¿ =========
with open(input_file, "r", encoding="utf-8") as f:
    lines = [line.strip() for line in f if line.strip()]

# ========= ç”¨ chunk åˆ†æ®µè«‹ AI åµæ¸¬ä¸»é¡Œåˆ‡é» =========
def batch_cut_by_chunk(lines, lines_per_chunk=60, stride=50, min_gap=6):
    split_indices = [0]
    idx = 0
    total_lines = len(lines)

    while idx < total_lines:
        chunk_lines = lines[idx:idx+lines_per_chunk]
        numbered_chunk = [f"[{i+idx}] {line}" for i, line in enumerate(chunk_lines)]
        chunk_text = "\n".join(numbered_chunk)

        prompt = f"""

---
{chunk_text}
---
è«‹é–±è®€ä»¥ä¸Šé€å­—ç¨¿ï¼Œæ‰¾å‡ºæ˜é¡¯çš„ä¸»é¡Œæ®µè½åˆ‡æ›çš„ä½ç½®ã€‚
è«‹åƒ…è¼¸å‡ºã€Œåˆ‡æ›é»çš„è¡Œè™Ÿã€ï¼ˆå³ä¸­æ‹¬è™Ÿå…§çš„åŸå§‹è¡Œè™Ÿï¼‰ï¼Œæ ¼å¼å¿…é ˆç¬¦åˆï¼š
åˆ‡æ›é»ï¼š12, 102

è«‹åš´æ ¼éµå®ˆï¼Œ**ä¸å‡†è¼¸å‡ºä»»ä½•å…¶ä»–æ–‡å­—èªªæ˜**ï¼Œå¦å‰‡è¦–ç‚ºéŒ¯èª¤ã€‚

"""
#        messages = [
#    {
#        "role": "user",
#        "content": "è«‹é–±è®€ä»¥ä¸‹é€å­—ç¨¿ï¼Œæ‰¾å‡ºæ˜é¡¯çš„ä¸»é¡Œæ®µè½åˆ‡æ›çš„ä½ç½®ã€‚\nè«‹åƒ…è¼¸å‡ºã€Œåˆ‡æ›é»çš„è¡Œè™Ÿã€ï¼ˆå³ä¸­æ‹¬è™Ÿå…§çš„åŸå§‹è¡Œè™Ÿï¼‰æ ¼å¼è¦å¯ä»¥è¢«r'åˆ‡æ›é»[:ï¼š]([\\d,\\s]+)'è®€å–ï¼Œè¼¸å‡ºæ ¼å¼åš´æ ¼éµå®ˆä»¥ä¸‹ï¼š"
#    },
#    {
#        "role": "assistant",
#        "content": "åˆ‡æ›é»ï¼š12, 102 é€™æ˜¯ä¸€æ®µåšç‰©é¤¨æˆ–é¡ä¼¼æ©Ÿæ§‹å…§éƒ¨æœƒè­°çš„éŒ„éŸ³ç´€éŒ„ï¼Œè¨è«–äº†é ç®—ç·Šç¸®å’Œæœªä¾†è¨ˆç•«ã€‚ä»¥ä¸‹æ˜¯å…§å®¹çš„æ‘˜è¦å’Œåˆ†æï¼š"
#    },
#    {
#        "role": "user",
#        "content": "æˆ‘èªªéä¸å‡†è¼¸å‡ºå…¶ä»–æ–‡å­—"
#    },
#    {
#        "role": "assistant",
#        "content": "åˆ‡æ›é»ï¼š45, 182,219"
#    },
#    {
#        "role": "user",
#        "content": "å¾ˆå¥½ç¹¼çºŒç¶­æŒé€™æ¨£çš„è¼¸å‡º"
#    }
#]
        messages=[]
        messages.append({"role": "user", "content": prompt})
        reply = ai_response(messages).strip()
        print(f"ğŸ§ª åˆ†æ chunk [{idx}:{idx+lines_per_chunk}] å›è¦†ï¼š{reply[:50]}...")

        match = re.search(r'åˆ‡æ›é»[:ï¼š]([\d,\s]+)', reply)
        if match:
            nums = match.group(1)
            new_points = [int(n.strip()) for n in nums.split(",") if n.strip().isdigit()]
            split_indices.extend(new_points)

        idx += stride

    # éæ¿¾éè¿‘çš„åˆ‡é»
    split_indices = sorted(set(split_indices + [len(lines)]))
    filtered = [split_indices[0]]
    for pt in split_indices[1:]:
        if pt - filtered[-1] >= min_gap:
            filtered.append(pt)

    return filtered
split_indices = batch_cut_by_chunk(lines)
# ========= åˆ‡æ®µä¸¦åŠ å…¥ padding =========
padding = 1
segments = []
for start_idx, end_idx in zip(split_indices[:-1], split_indices[1:]):
    real_start = max(0, start_idx - padding)
    real_end = end_idx  # âœ… ä¸åŠ å°¾ç«¯ paddingï¼Œé¿å…é‡è¤‡
    segments.append(lines[real_start:real_end])

# ========= æª”åæ¸…ç† =========
def sanitize_filename(text):
    text = re.sub(r'[\\/*?:"<>|ï¼ˆï¼‰()ã€ã€‘ã€Œã€ã€ï¼Œã€‚ï¼ï¼Ÿ~`\'\s]+', '_', text)
    text = re.sub(r'[^\w\u4e00-\u9fff]', '', text)
    return text[:20].strip("_")

# ========= å¯«å…¥ Markdown =========
for i, segment in enumerate(tqdm(segments, desc="å¯«å…¥ä¸»é¡Œæ®µè½"), 1):
    text_block = "\n".join(segment)
    summary_input = text_block[:MAX_TEXT_LEN]
    summary_prompt = f"""è«‹å¯«å‡ºé€™ä¸€æ®µæœ€é‡è¦çš„äº‹æƒ…ï¼Œä¸è¦åŠ æ¨™é»æˆ–è§£é‡‹ï¼š
---
{summary_input}
---
ä¸»é¡Œï¼š"""
    messages = [{"role": "user", "content": summary_prompt}]
    topic_title = ai_response(messages, max_tokens=1000)
    filename_title = sanitize_filename(topic_title)
    filename = f"{i:02d}_{filename_title}.md"
    filepath = os.path.join(output_dir, filename)

    with open(filepath, "w", encoding="utf-8") as f:
        f.write(f"# {topic_title}\n\n")
        f.write(text_block)

    print(f"âœ… å¯«å…¥ï¼š{filename}")

print(f"\nğŸ‰ å…±åˆ‡å‡º {len(segments)} æ®µä¸»é¡Œï¼Œå„²å­˜åœ¨ï¼š{output_dir}")
print("time cost", time.time() - starttime)
