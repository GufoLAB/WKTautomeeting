#é€™æ˜¯ä¸€å€‹é€å¥é–±è®€æ–‡ç« æ¯æ¬¡æ–°å¢å¹¾å€‹åŸæ–‡ä¸­çš„å¹¾è¡Œè®“ï¼¡ï¼©å›å‚³true or falseä¾†å»ºç«‹åˆ†æ®µç¨‹å¼
import os
import re
import argparse
import time
from dotenv import load_dotenv
from tqdm import tqdm
starttime=time.time()
# åƒæ•¸è¨­å®š
parser = argparse.ArgumentParser(description="ç”¨ GPT å°‡é€å­—ç¨¿ä¾ä¸»é¡Œåˆ†æ®µä¸¦è¼¸å‡º md æª”")
parser.add_argument("input_file", help="è¼¸å…¥é€å­—ç¨¿æª”æ¡ˆï¼ˆæ¯è¡Œæ ¼å¼ç‚º speakerX: èªªè©±å…§å®¹ï¼‰")
args = parser.parse_args()

input_file = args.input_file
MAX_TEXT_LEN = 3000
#æœ‰åˆ†å…©å€‹ï¼¡ï¼©ä¸€å€‹åˆ‡æ–·ä¸€å€‹å¯«çµè«– å¯ä»¥search modelä¿®æ”¹


# è¼¸å‡ºè³‡æ–™å¤¾
output_dir = os.path.splitext(os.path.basename(input_file))[0] + "_topics"
os.makedirs(output_dir, exist_ok=True)


# è®€å…¥é€å­—ç¨¿
with open(input_file, "r", encoding="utf-8") as f:
    lines = [line.strip() for line in f if line.strip()]

# ğŸ” ä¸»é¡Œç”Ÿé•·å¼åˆ‡æ®µ
start = 0
topic_indices = [0]
step = 3
initial_len = 4
# -----------------------------------------------------------------------------------------------
# -----------------------------------------------------------------------------------------------
# -----------------------------------------------------------------------------------------------


import os, re
from dotenv import load_dotenv
load_dotenv()

from openai import OpenAI
import ollama
from zhconv_rs import zhconv

from config import BACK_END_MODEL, AI_MODEL, OLLAMA_URL

openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

# è¨­å®š çµ±ä¸€system promptï¼Œä¸¦å‹•æ…‹è¨˜éŒ„ç‰ˆæœ¬è³‡è¨Š
#import system_prompt
#global prompt_choice, prompt_version
#prompt_function = system_prompt.system_prompt  # å–å¾— system_prompt å‡½å¼
#prompt_choice = prompt_function()  # åŸ·è¡Œå‡½å¼ä»¥å–å¾—æç¤ºå…§å®¹
#prompt_version = f"{prompt_function.__module__}.{prompt_function.__name__}"


def ai_response(conversation_history, max_tokens=1000):
    if BACK_END_MODEL == 'openai':
        response = openai_client.chat.completions.create(
            model=AI_MODEL, 
            messages=conversation_history
        )
        print("model = openai")
        assistant_reply = response.choices[0].message.content
    elif BACK_END_MODEL == 'ollama':
        response = ollama.Client(host=OLLAMA_URL).chat(
            model=AI_MODEL, 
            messages=conversation_history
        )
        print("model = ollama "+str(AI_MODEL))
        assistant_reply = response['message']['content'].strip()
        if AI_MODEL.startswith("deepseek"):
            assistant_reply = re.sub(r'<think>(.*)</think>', '', assistant_reply, flags=re.DOTALL).strip()
    assistant_reply = zhconv(assistant_reply, "zh-tw")
    return assistant_reply
# -----------------------------------------------------------------------------------------------
# -----------------------------------------------------------------------------------------------
# -----------------------------------------------------------------------------------------------


while start < len(lines):
    cur_len = initial_len
    while start + cur_len < len(lines):
        block = "\n".join(lines[start:start+cur_len])
        prompt = f"""ä»¥ä¸‹æ˜¯ä¸€æ®µé€å­—ç¨¿æ®µè½ã€‚è«‹åˆ¤æ–·é€™æ®µæ˜¯å¦åªæœ‰æè¿°ä¸€å€‹è¨è«–ä¸»é¡Œã€‚
å¦‚æœè¨è«–å…§å®¹éƒ½æ˜¯åŒä¸€å€‹ä¸»é¡Œè«‹å›å‚³ Trueï¼è‹¥è¨è«–å…§å®¹åŒ…å«å¤šå€‹ä¸»é¡Œï¼Œå‰‡è«‹å›å‚³ Falseï¼š
---
{block}
---
è¼¸å‡ºåªæœ‰True or Falseå…©ç¨®ï¼Œå…¶ä»–æ–‡å­—éƒ½ä¸éœ€è¦ï¼š"""

        messages = [{"role": "user", "content": prompt}]
        print(f"\nâ³ æª¢æŸ¥æ®µè½ [{start}:{start+cur_len}]")
        reply = ai_response(messages, max_tokens=1000).strip().lower()
        print(f"ğŸ” å›è¦†ï¼š{reply}")

        if "false" in reply or cur_len >= 20:
            break
        cur_len += step

    # å®‰å…¨ä¿éšœï¼šè¬ä¸€æ¨¡å‹å¤ªå¿«å› falseï¼Œè‡³å°‘åˆ‡ä¸‹ä¸€æ®µï¼Œä¸å¡æ­»
    if cur_len == initial_len:
        print("âš ï¸ æ¨¡å‹ææ—©å› Falseï¼Œå¼·åˆ¶åˆ‡å‡ºä¸€å°æ®µ")
        cur_len = step

    start += cur_len
    topic_indices.append(start)
    print(f"âœ… æ–°åˆ‡é»ï¼š{start}")


# âœ… æ ¹æ“šåˆ‡é»åˆ‡æ®µï¼ŒåŠ å…¥ä¸Šä¸‹æ–‡ padding
padding = 2
topic_indices = sorted(set(topic_indices))
segments = []

for i in range(len(topic_indices)):
    seg_start = topic_indices[i]
    seg_end = topic_indices[i+1] if i+1 < len(topic_indices) else len(lines)

    real_start = max(0, seg_start - padding)
    real_end = min(len(lines), seg_end + padding)
    segments.append(lines[real_start:real_end])

# ğŸ”– æ¸…ç†éŒ¯èª¤å­—å…ƒ
def sanitize_filename(text):
    # ç§»é™¤æ‰€æœ‰ä¸åˆæ³•æª”åå­—å…ƒ
    text = re.sub(r'[\\/*?:"<>|ï¼ˆï¼‰()ã€ã€‘ã€Œã€ã€ï¼Œã€‚ï¼ï¼Ÿ~`\'\s]+', '_', text)
    # åƒ…ä¿ç•™ä¸­è‹±æ–‡èˆ‡æ•¸å­—ï¼ˆé¿å… emojiï¼‰
    text = re.sub(r'[^\w\u4e00-\u9fff]', '', text)
    # é™åˆ¶é•·åº¦
    return text[:20].strip("_")

# ğŸ“ å¯«å…¥æ¯æ®µï¼‹ä¸»é¡Œæ‘˜è¦
for i, segment in enumerate(tqdm(segments, desc="å¯«å…¥ä¸»é¡Œæ®µè½"), 1):
    text_block = "\n".join(segment)
    summary_input = text_block[:MAX_TEXT_LEN]

    summary_prompt = f"""è«‹å¯«å‡ºé€™ä¸€æ®µæœ€é‡è¦çš„äº‹æƒ…ï¼Œä¸è¦åŠ æ¨™é»æˆ–è§£é‡‹ï¼š
---
{summary_input}
---
ä¸»é¡Œï¼š"""


    messages=[{"role": "user", "content": summary_prompt}]
    
    topic_title = ai_response(messages, max_tokens=1000)
    filename_title = sanitize_filename(topic_title)
    filename = f"{i:02d}_{filename_title}.md"
    filepath = os.path.join(output_dir, filename)

    with open(filepath, "w", encoding="utf-8") as f:
        f.write(f"# {topic_title}\n\n")
        f.write(text_block)

    print(f"âœ… å¯«å…¥ï¼š{filename}")

print(f"\nğŸ‰ å…±åˆ‡å‡º {len(segments)} æ®µä¸»é¡Œï¼Œå„²å­˜åœ¨ï¼š{output_dir}")
Endtime=time.time()
print("time cost",Endtime-starttime)