#!/usr/bin/env python3
"""
5merge_with_continuous_writing.py - ä½¿ç”¨ Continuous Writing æŠ€æœ¯æ•´åˆå¤šä¸»é¢˜ MD

è¿™æ˜¯æ­¥éª¤ 7.4 - ä½¿ç”¨ continuous writing æŠ€æœ¯å°†å¤šä¸ªä¸»é¢˜ MD è‡ªç„¶æ•´åˆæˆå•ä¸€å®Œæ•´æŠ¥å‘Š

ç‰¹ç‚¹ï¼š
- ä½¿ç”¨ç±»ä¼¼ continuous_writing.py çš„æŠ€æœ¯
- é€ä¸ªä¸»é¢˜è‡ªç„¶è¡”æ¥
- ä¿ç•™æ‰€æœ‰é‡è¦ç»†èŠ‚
- è¾“å‡ºæµç•…çš„ä¸“ä¸šä¼šè®®è®°å½•

ç”¨æ³•ï¼š
python 5merge_with_continuous_writing.py --input-dir /path/to/md/files --output integrated_report.md

æˆ–è€…æ¥ç»­ main_part2_test.py çš„ç»“æœï¼š
python 5merge_with_continuous_writing.py --from-judgment /path/to/topic_judgments.json --output integrated_report.md
"""

import argparse
import os
import re
import json
import time
import threading
from pathlib import Path
from typing import List, Dict
import ollama
from zhconv_rs import zhconv
from config import BACK_END_MODEL, AI_MODEL, OLLAMA_URL


# ==================== å·¥å…·å‡½æ•° ====================

def print_dot(stop_event):
    """æ˜¾ç¤ºè¿›åº¦ç‚¹"""
    while not stop_event.is_set():
        print('.', end='', flush=True)
        time.sleep(0.8)


def ai_response(messages, max_tokens=2000):
    """è°ƒç”¨ AI æ¨¡å‹"""
    if BACK_END_MODEL == 'openai':
        from openai import OpenAI
        client = OpenAI()
        resp = client.chat.completions.create(
            model=AI_MODEL,
            messages=messages,
            max_tokens=max_tokens
        )
        text = resp.choices[0].message.content
    else:
        client = ollama.Client(host=OLLAMA_URL)
        resp = client.chat(
            model=AI_MODEL,
            messages=messages
        )
        text = resp['message']['content']

    # ç§»é™¤ deepseek çš„ <think> æ ‡ç­¾
    if AI_MODEL.startswith('deepseek'):
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)

    return zhconv(text.strip(), 'zh-tw')


# ==================== Continuous Writing æ•´åˆ ====================

SYSTEM_PROMPT_INIT = """ä½ æ˜¯å°ˆæ¥­çš„æœƒè­°è¨˜éŒ„æ’°å¯«å°ˆå®¶ã€‚

è«‹æ ¹æ“šæä¾›çš„æœƒè­°ä¸»é¡Œå…§å®¹ï¼Œæ’°å¯«ä¸€ä»½å°ˆæ¥­çš„æœƒè­°è¨˜éŒ„é–‹é ­éƒ¨åˆ†ã€‚

è¦æ±‚ï¼š
1. ä½¿ç”¨æ­£å¼çš„æœƒè­°è¨˜éŒ„æ ¼å¼
2. è©³ç´°åˆ—å‡ºæ‰€æœ‰é‡è¦ç´°ç¯€ï¼ˆäººäº‹æ™‚åœ°ç‰©ã€é‡‘é¡ã€æ™‚ç¨‹ç­‰ï¼‰
3. ä½¿ç”¨ Markdown æ¢åˆ—å¼æ ¼å¼
4. ä¿æŒå®¢è§€å°ˆæ¥­çš„èªæ°£
5. ä¸è¦æ·»åŠ åŸæ–‡æ²’æœ‰çš„å…§å®¹

ç›´æ¥è¼¸å‡ºå…§å®¹ï¼Œä¸éœ€è¦å…¶ä»–èªªæ˜ã€‚
"""


SYSTEM_PROMPT_CONTINUE = """ä½ æ˜¯å°ˆæ¥­çš„æœƒè­°è¨˜éŒ„æ•´åˆå°ˆå®¶ã€‚

æˆ‘æœƒçµ¦ä½ ï¼š
1. å‰æ–‡çš„æœ€å¾Œ 15 è¡Œ
2. æ–°çš„æœƒè­°ä¸»é¡Œå…§å®¹

è«‹å°‡æ–°ä¸»é¡Œè‡ªç„¶åœ°æ¥çºŒåˆ°å‰æ–‡ï¼Œè¦æ±‚ï¼š
1. **ä¿æŒæ‰€æœ‰é‡è¦ç´°ç¯€**ï¼ˆäººäº‹æ™‚åœ°ç‰©ã€é‡‘é¡ã€æ™‚ç¨‹ç­‰ï¼‰
2. **è‡ªç„¶çš„æ®µè½éŠœæ¥**ï¼ˆä¸è¦çªå…€ï¼Œé©ç•¶çš„éæ¸¡èªå¥ï¼‰
3. **çµ±ä¸€çš„æ ¼å¼é¢¨æ ¼**ï¼ˆèˆ‡å‰æ–‡ä¿æŒä¸€è‡´ï¼‰
4. **å¯ä»¥é©ç•¶ä¿®æ”¹æœ€å¾Œ 15 è¡Œ**ï¼Œä½¿éŠœæ¥æ›´æµæš¢
5. ä½¿ç”¨ Markdown æ ¼å¼
6. ä¸è¦æ·»åŠ å¤šé¤˜çš„èªªæ˜æ–‡å­—

ç›´æ¥è¼¸å‡ºæ•´åˆå¾Œçš„æœ€çµ‚å…§å®¹ï¼ˆåŒ…å«ä¿®æ”¹å¾Œçš„å‰æ–‡æœ«æ®µ + æ–°å…§å®¹ï¼‰ã€‚
"""


def initialize_report(first_file: Path, output_path: Path, total_topics: int):
    """
    åˆå§‹åŒ–æŠ¥å‘Š - ä½¿ç”¨ç¬¬ä¸€ä¸ªä¸»é¢˜
    """
    print(f"\nğŸ”„ åˆå§‹åŒ–å ±å‘Š")
    print(f"  ğŸ“„ ä½¿ç”¨ç¬¬ä¸€å€‹ä¸»é¡Œï¼š{first_file.name}")

    # è¯»å–ç¬¬ä¸€ä¸ªä¸»é¢˜
    with open(first_file, 'r', encoding='utf-8') as f:
        first_content = f.read().strip()

    # æ„å»º prompt
    user_prompt = f"""é€™æ˜¯æœƒè­°çš„ç¬¬ä¸€å€‹ä¸»é¡Œï¼ˆå…± {total_topics} å€‹ä¸»é¡Œï¼‰ï¼š

{first_content}

è«‹æ’°å¯«æœƒè­°è¨˜éŒ„çš„é–‹é ­éƒ¨åˆ†ã€‚"""

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT_INIT},
        {"role": "user", "content": user_prompt}
    ]

    # AI å¤„ç†
    print(f"  ğŸ¤– AI æ­£åœ¨è™•ç†", end='')
    stop_event = threading.Event()
    dot_thread = threading.Thread(target=print_dot, args=(stop_event,))
    dot_thread.start()

    try:
        initial_content = ai_response(messages, max_tokens=2000)
    finally:
        stop_event.set()
        dot_thread.join()

    print(f" âœ…")

    # æ·»åŠ æŠ¥å‘Šå¤´éƒ¨
    final_content = f"""# å®Œæ•´æœƒè­°è¨˜éŒ„

**æœƒè­°ä¸»é¡Œæ•¸ï¼š** {total_topics}

---

{initial_content}
"""

    # å†™å…¥æ–‡ä»¶
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(final_content)

    print(f"  ğŸ’¾ åˆå§‹å ±å‘Šå·²å»ºç«‹")


def integrate_next_topic(output_path: Path, next_file: Path, topic_index: int, total_topics: int):
    """
    æ•´åˆä¸‹ä¸€ä¸ªä¸»é¢˜ - ä½¿ç”¨ continuous writing æŠ€æœ¯
    """
    print(f"\nğŸ”„ æ•´åˆä¸»é¡Œ {topic_index}/{total_topics}")
    print(f"  ğŸ“„ {next_file.name}")

    # è¯»å–å½“å‰æŠ¥å‘Šçš„æœ€å 15 è¡Œ
    with open(output_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    last_15_lines = ''.join(lines[-15:] if len(lines) >= 15 else lines)

    # è¯»å–æ–°ä¸»é¢˜å†…å®¹
    with open(next_file, 'r', encoding='utf-8') as f:
        next_content = f.read().strip()

    # æ£€æŸ¥å­—æ•°
    total_chars = len(last_15_lines) + len(next_content)
    print(f"  ğŸ“Š è¼¸å…¥å­—æ•¸ï¼šå‰æ–‡ {len(last_15_lines)} + æ–°å…§å®¹ {len(next_content)} = {total_chars} å­—")

    if total_chars > 4000:
        print(f"  âš ï¸  è­¦å‘Šï¼šç¸½å­—æ•¸è¶…é 4000ï¼Œå¯èƒ½å½±éŸ¿ Gemma3 è™•ç†æ•ˆæœ")

    # æ„å»º prompt
    user_prompt = f"""å‰æ–‡æœ€å¾Œ 15 è¡Œï¼š

{last_15_lines}

---

æ–°çš„æœƒè­°ä¸»é¡Œï¼š

{next_content}

---

è«‹è‡ªç„¶åœ°å°‡æ–°ä¸»é¡Œæ¥çºŒåˆ°å‰æ–‡ï¼Œè¼¸å‡ºå®Œæ•´çš„æœ€çµ‚å…§å®¹ã€‚"""

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT_CONTINUE},
        {"role": "user", "content": user_prompt}
    ]

    # AI å¤„ç†
    print(f"  ğŸ¤– AI æ­£åœ¨æ•´åˆ", end='')
    stop_event = threading.Event()
    dot_thread = threading.Thread(target=print_dot, args=(stop_event,))
    dot_thread.start()

    try:
        merged_content = ai_response(messages, max_tokens=2000)
    finally:
        stop_event.set()
        dot_thread.join()

    print(f" âœ…")

    # æ›´æ–°æ–‡ä»¶ï¼šæ›¿æ¢æœ€å 15 è¡Œ + æ·»åŠ æ–°å†…å®¹
    if len(lines) >= 15:
        new_lines = lines[:-15] + [merged_content + '\n']
    else:
        new_lines = [merged_content + '\n']

    with open(output_path, 'w', encoding='utf-8') as f:
        f.writelines(new_lines)

    print(f"  ğŸ’¾ å·²æ›´æ–°å ±å‘Š")


def continuous_merge(sorted_files: List[Path], output_path: Path):
    """
    ä¸»æµç¨‹ï¼šä½¿ç”¨ continuous writing æ•´åˆæ‰€æœ‰ä¸»é¢˜
    """
    total_topics = len(sorted_files)

    print("="*60)
    print("ğŸš€ é–‹å§‹ Continuous Writing æ•´åˆ")
    print("="*60)
    print(f"ğŸ“Š ç¸½å…± {total_topics} å€‹ä¸»é¡Œ")
    print(f"ğŸ“„ è¼¸å‡ºï¼š{output_path}")

    # æ­¥éª¤ 1ï¼šåˆå§‹åŒ–ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªä¸»é¢˜ï¼‰
    initialize_report(sorted_files[0], output_path, total_topics)

    # æ­¥éª¤ 2ï¼šé€ä¸ªæ•´åˆå‰©ä½™ä¸»é¢˜
    for i, next_file in enumerate(sorted_files[1:], 2):
        integrate_next_topic(output_path, next_file, i, total_topics)

    print("\n" + "="*60)
    print("âœ… Continuous Writing æ•´åˆå®Œæˆï¼")
    print("="*60)

    # æ˜¾ç¤ºç»Ÿè®¡
    with open(output_path, 'r', encoding='utf-8') as f:
        final_lines = f.readlines()

    file_size = output_path.stat().st_size / 1024
    print(f"ğŸ“„ æœ€çµ‚å ±å‘Šï¼š{output_path}")
    print(f"ğŸ“Š ç¸½è¡Œæ•¸ï¼š{len(final_lines)}")
    print(f"ğŸ“¦ æª”æ¡ˆå¤§å°ï¼š{file_size:.1f} KB")


# ==================== ä¸»ç¨‹åº ====================

def main():
    parser = argparse.ArgumentParser(
        description='ä½¿ç”¨ Continuous Writing æŠ€æœ¯æ•´åˆå¤šä¸»é¢˜ MD æ–‡ä»¶'
    )

    # è¾“å…¥æ–¹å¼ 1ï¼šæŒ‡å®šç›®å½•
    parser.add_argument('--input-dir', help='åŒ…å« MD æ–‡ä»¶çš„ç›®å½•')

    # è¾“å…¥æ–¹å¼ 2ï¼šä» judgment ç»“æœè¯»å–
    parser.add_argument('--from-judgment', help='ä» topic_judgments.json è¯»å–å·²è¿‡æ»¤çš„æ–‡ä»¶åˆ—è¡¨')

    # è¾“å‡º
    parser.add_argument('--output', default='final_report_integrated.md',
                        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')

    args = parser.parse_args()

    start_time = time.time()

    # ==================== è·å–æ–‡ä»¶åˆ—è¡¨ ====================

    if args.from_judgment:
        # æ–¹å¼ 2ï¼šä» judgment JSON è¯»å–
        judgment_file = Path(args.from_judgment)
        if not judgment_file.exists():
            print(f"âŒ Judgment æ–‡ä»¶ä¸å­˜åœ¨ï¼š{judgment_file}")
            return

        with open(judgment_file, 'r', encoding='utf-8') as f:
            judgments = json.load(f)

        # è·å–ä¿ç•™çš„æ–‡ä»¶
        input_dir = judgment_file.parent
        kept_files = [
            input_dir / j['filename']
            for j in judgments if j['keep']
        ]

        print(f"ğŸ“ å¾ judgment çµæœè®€å–ï¼š{judgment_file}")
        print(f"ğŸ“Š ä¿ç•™ {len(kept_files)} å€‹ä¸»é¡Œ")

        # éœ€è¦æ’åºä¿¡æ¯
        # å‡è®¾åœ¨åŒä¸€ç›®å½•ä¸‹æŸ¥æ‰¾ final_report.md ä¸­çš„é¡ºåº
        final_report = input_dir / 'final_report.md'
        if final_report.exists():
            with open(final_report, 'r', encoding='utf-8') as f:
                content = f.read()

            # æå–é¡ºåº
            ordered_filenames = []
            for match in re.finditer(r'## \d+\. (.+)', content):
                topic_name = match.group(1).strip()
                # åŒ¹é…æ–‡ä»¶å
                for f in kept_files:
                    if topic_name.replace(' ', '_') in f.name or \
                       topic_name.replace(' ', '') in f.name:
                        if f not in ordered_filenames:
                            ordered_filenames.append(f)
                            break

            # æ·»åŠ æœªåŒ¹é…çš„æ–‡ä»¶
            for f in kept_files:
                if f not in ordered_filenames:
                    ordered_filenames.append(f)

            sorted_files = ordered_filenames
        else:
            # æ²¡æœ‰æ’åºä¿¡æ¯ï¼ŒæŒ‰æ–‡ä»¶åæ’åº
            sorted_files = sorted(kept_files, key=lambda x: x.name)

    elif args.input_dir:
        # æ–¹å¼ 1ï¼šä»ç›®å½•è¯»å–
        input_dir = Path(args.input_dir)
        if not input_dir.exists():
            print(f"âŒ ç›®éŒ„ä¸å­˜åœ¨ï¼š{input_dir}")
            return

        # è·å–æ‰€æœ‰ MD æ–‡ä»¶ï¼ˆæ’é™¤ç³»ç»Ÿæ–‡ä»¶ï¼‰
        all_md_files = list(input_dir.glob('*.md'))
        exclude_keywords = ['CLAUDE', 'README', 'PATENT', 'cleaned', 'final', 'integrated']

        md_files = [
            f for f in all_md_files
            if not any(exc in f.name for exc in exclude_keywords)
        ]

        if not md_files:
            print(f"âŒ åœ¨ {input_dir} ä¸­æœªæ‰¾åˆ° MD æ–‡ä»¶")
            return

        # æŒ‰æ–‡ä»¶åæ’åºï¼ˆç®€å•æ–¹å¼ï¼‰
        sorted_files = sorted(md_files, key=lambda x: x.name)

        print(f"ğŸ“ è¼¸å…¥ç›®éŒ„ï¼š{input_dir}")
        print(f"ğŸ“„ æ‰¾åˆ° {len(sorted_files)} å€‹ MD æ–‡ä»¶")

    else:
        print("âŒ è«‹æŒ‡å®š --input-dir æˆ– --from-judgment")
        return

    # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
    print(f"\nğŸ“ è™•ç†é †åºï¼š")
    for i, f in enumerate(sorted_files, 1):
        size_kb = f.stat().st_size / 1024
        print(f"  {i}. {f.name} ({size_kb:.1f} KB)")

    # ==================== ç¡®å®šè¾“å‡ºè·¯å¾„ ====================

    if args.output == 'final_report_integrated.md':
        # é»˜è®¤è¾“å‡ºåˆ° input_dir
        if args.from_judgment:
            output_path = judgment_file.parent / args.output
        else:
            output_path = input_dir / args.output
    else:
        output_path = Path(args.output)

    # ==================== æ‰§è¡Œæ•´åˆ ====================

    continuous_merge(sorted_files, output_path)

    # ==================== æ€»ç»“ ====================

    elapsed = time.time() - start_time
    print(f"\nâ±ï¸  ç¸½è€—æ™‚ï¼š{elapsed:.1f} ç§’")
    print(f"\nğŸ’¡ æç¤ºï¼š")
    print(f"  - å¯ä»¥ç”¨æ–‡æœ¬ç·¨è¼¯å™¨æ‰“é–‹æŸ¥çœ‹ï¼š{output_path}")
    print(f"  - èˆ‡åŸå§‹æ‹¼æ¥ç‰ˆæœ¬æ¯”è¼ƒï¼šdiff final_report.md {output_path.name}")


if __name__ == '__main__':
    main()
