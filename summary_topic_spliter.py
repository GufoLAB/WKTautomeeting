"""
é€™å€‹ç¨‹å¼çš„åŠŸèƒ½æ˜¯ï¼š
1. è®€å–ä¸€å€‹å«æœ‰æ‘˜è¦æ¬„ä½çš„ CSV æª”æ¡ˆã€‚
2. æ¯ 8 è¡Œæ‘˜è¦ç‚ºä¸€çµ„ï¼Œä½¿ç”¨ GPT æ¨¡å‹æ‰¾å‡ºä¸»é¡Œè½‰æ›çš„ã€Œåˆ‡æ›é»ã€è¡Œè™Ÿã€‚
3. æ ¹æ“šåˆ‡æ›é»å°‡è³‡æ–™åˆ‡æ®µï¼Œæ¯æ®µå¦å­˜æˆä¸€å€‹ CSV æª”ã€‚
4. æ¯æ®µå…§å®¹é€çµ¦ GPT ç”Ÿå‡ºä¸€å€‹åå­—ä»¥å…§çš„ä¸»é¡Œæ¨™é¡Œï¼Œä½œç‚ºæª”åã€‚
5. æœ€çµ‚å°‡æ‰€æœ‰åˆ‡æ®µå­˜å…¥ä¸€å€‹æ–°å»ºç«‹çš„è³‡æ–™å¤¾ä¸­ã€‚

è¼¸å…¥ï¼š
- CSV æª”æ¡ˆè·¯å¾‘ï¼ˆéœ€å«æœ‰ `summary` æ¬„ä½ï¼‰

è¼¸å‡ºï¼š
- ä¸€å€‹åç‚º `<åŸå§‹æª”æ¡ˆå>_topic_blocks/` çš„è³‡æ–™å¤¾
- å…§å«æ•¸å€‹ä»¥ä¸»é¡Œå‘½åçš„ `.csv` æª”æ¡ˆï¼Œæ¯æª”ä»£è¡¨ä¸€æ®µä¸»é¡Œå€å¡Š

åŸ·è¡Œæ–¹å¼ï¼ˆç¯„ä¾‹ï¼‰ï¼š
    python split_topics.py my_summary.csv
"""
import os
import re
import argparse
import time
import pandas as pd
from dotenv import load_dotenv
from tqdm import tqdm
from zhconv_rs import zhconv
import ollama
from config import BACK_END_MODEL, AI_MODEL, OLLAMA_URL
from openai import OpenAI

# ========= INIT =========
starttime = time.time()
parser = argparse.ArgumentParser(description="ç”¨ GPT åˆ†æ CSV æ‘˜è¦åˆ—è¡¨çš„ä¸»é¡Œè½‰æ›é»")
parser.add_argument("input_file", help="è¼¸å…¥ CSV æª”æ¡ˆï¼ˆæ¬„ä½éœ€åŒ…å« summaryï¼‰")
args = parser.parse_args()

input_file = args.input_file  # ğŸ”¸é€™ä¸€è¡Œä¸èƒ½å°‘ï¼
input_dir = os.path.dirname(input_file)
output_dir = os.path.join(input_dir, os.path.splitext(os.path.basename(input_file))[0] + "_topic_blocks")
os.makedirs(output_dir, exist_ok=True)

# ========= AI åˆå§‹åŒ– =========
load_dotenv()
openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

def ai_response(conversation_history, max_tokens=1000):
    if BACK_END_MODEL == 'openai':
        response = openai_client.chat.completions.create(
            model=AI_MODEL,
            messages=conversation_history
        )
        assistant_reply = response.choices[0].message.content
    elif BACK_END_MODEL == 'ollama':
        response = ollama.Client(host=OLLAMA_URL).chat(
            model=AI_MODEL,
            messages=conversation_history
        )
        assistant_reply = response['message']['content'].strip()
        if AI_MODEL.startswith("deepseek"):
            assistant_reply = re.sub(r'<think>(.*)</think>', '', assistant_reply, flags=re.DOTALL).strip()
    assistant_reply = zhconv(assistant_reply, "zh-tw")
    return assistant_reply

# ========= è®€å…¥ CSV =========
df = pd.read_csv(input_file)
summaries = df['summary'].tolist()

def detect_topic_transitions(summaries, block_size=8, min_block_len=4):
    """
    æ ¹æ“šé€æ®µæ‘˜è¦ï¼Œç”¨ GPT æ¨¡å‹æ‰¾å‡ºä¸»é¡Œåˆ‡æ›é»ï¼Œå›å‚³åˆ‡æ®µ index æ¸…å–®ã€‚
    æ¯æ¬¡å¾ä¸Šä¸€å€‹åˆ‡é»é–‹å§‹å‘ä¸‹åˆ†æï¼Œä¸é‡è¤‡ã€ä¸éºæ¼ã€‚
    """
    split_points = []
    current_index = 0

    while current_index < len(summaries):
        sub = summaries[current_index:current_index + block_size]
        if not sub:
            break

        # æº–å‚™ promptï¼Œå¾ 1 é–‹å§‹æ¨™è™Ÿ
        prompt_lines = [f"[{current_index + j + 1}] {line}" for j, line in enumerate(sub)]
        chunk_text = "\n".join(prompt_lines)

        prompt = f"""{chunk_text}
---
è«‹é–±è®€ä»¥ä¸Šé€æ®µæ‘˜è¦ï¼Œä»¥ã€Œä¸»é¡Œæ˜¯å¦æ˜é¡¯æ”¹è®Šã€ç‚ºåˆ¤æº–ï¼Œåˆ¤æ–·ä¸»é¡Œæ®µè½çš„åˆ‡æ›ä½ç½®ã€‚
è‹¥æ®µè½å…§å®¹åƒ…æ˜¯æ•˜è¿°ç´°ç¯€ã€åŒä¸€è¨è«–è„ˆçµ¡å»¶ä¼¸ï¼Œè«‹å‹¿è¦–ç‚ºåˆ‡æ›é»ã€‚
åƒ…åœ¨æ®µè½ä¸»è»¸æ˜é¡¯æ”¹è®Šï¼ˆä¾‹å¦‚å¾é ç®—æ”¹è«‡äººåŠ›ã€å¾æ´»å‹•æ”¹è«‡æ³•è¦ï¼‰æ™‚æ‰åˆ‡æ›ï¼Œä¸”æ¯æ®µè½éœ€é”ä¸‰å¥ä»¥ä¸Šæ‰å¯åˆ‡æ›ã€‚

è«‹åƒ…è¼¸å‡ºã€Œåˆ‡æ›é»çš„è¡Œè™Ÿã€ï¼ˆå³æ¯çµ„æ‘˜è¦é–‹é ­å‰çš„æ•¸å­—ï¼‰ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
åˆ‡æ›é»ï¼š4, 7
ä¸å‡†è¼¸å‡ºå…¶ä»–æ–‡å­—æˆ–å¥é»ã€‚
---
"""

        messages = [{"role": "user", "content": prompt}]
        reply = ai_response(messages)
        print(f"ğŸ§ª åˆ†ææ‘˜è¦æ®µ [{current_index}â€“{current_index + block_size}] å›è¦†ï¼š{reply.strip()}")

        match = re.search(r'åˆ‡æ›é»[:ï¼š]([\d,\s]+)', reply)
        if match:
            # å°‡ GPT å›å‚³çš„è¡Œè™Ÿè½‰ç‚º summaries ä¸­çš„ indexï¼ˆ1-based â†’ 0-basedï¼‰
            raw_nums = match.group(1)
            raw_points = [int(n.strip()) for n in raw_nums.split(",") if n.strip().isdigit()]
            adjusted_points = [current_index + (p - 1) for p in raw_points if 1 <= p <= len(sub)]
            print(f"ğŸ‘‰ åˆ‡é» index: {adjusted_points}")

            # éæ¿¾ï¼šèˆ‡å‰æ®µè·é›¢è‡³å°‘ min_block_len è¡Œ
            valid_points = []
            prev = current_index
            for p in sorted(adjusted_points):
                if p - prev >= min_block_len:
                    valid_points.append(p)
                    prev = p

            if valid_points:
                # å–ç¬¬ä¸€å€‹æœ‰æ•ˆåˆ‡é»ä½œç‚ºä¸‹ä¸€æ®µé–‹å§‹
                split_points.append(valid_points[0])
                current_index = valid_points[0]
            else:
                print(f"âš ï¸ æ²’æœ‰æœ‰æ•ˆåˆ‡é»ï¼Œè·³é {current_index}ï½{current_index + block_size}")
                current_index += block_size
        else:
            print(f"âš ï¸ ç„¡åˆ‡é»æ ¼å¼æ–¼æ®µè½ {current_index}â€“{current_index + block_size}ï¼Œè·³é")
            current_index += block_size

    # è£œä¸Šæœ€å¾Œä¸€æ®µçµå°¾
    split_points.append(len(summaries))
    return sorted(set(split_points))




split_indices = detect_topic_transitions(summaries)

# ========= åˆ‡æ®µä¸¦å¯«å‡ºæ¯æ®µ =========
blocks = []
for idx, (start_idx, end_idx) in enumerate(zip([0] + split_indices[:-1], split_indices)):
    sub_df = df.iloc[start_idx:end_idx].copy()
    blocks.append(sub_df)

    # åˆä½µå…§å®¹æ‘˜è¦ä¸¦ç”¢ç”Ÿä¸»é¡Œ
    combined_text = "\n".join(sub_df['summary'].tolist())[:3000]
    topic_prompt = f"""{combined_text}
---
è«‹ç‚ºä»¥ä¸Šå…§å®¹æ‘˜è¦å‡º**åå€‹å­—ä»¥å…§çš„ä¸»é¡Œåç¨±**ï¼ˆä¸è¦åŠ æ¨™é»ã€ä¸è¦å¤šé¤˜è§£é‡‹ï¼‰ï¼š
ä¸»é¡Œï¼š
"""
    messages = [{"role": "user", "content": topic_prompt}]
    topic_title = ai_response(messages).strip()

    # æ¸…ç†æ¨™é¡Œæˆæª”åç”¨
    filename_title = re.sub(r'[\\/*?:"<>|ï¼ˆï¼‰()ã€ã€‘ã€Œã€ã€ï¼Œã€‚ï¼ï¼Ÿ~`\'\s]+', '_', topic_title)
    filename_title = re.sub(r'[^\w\u4e00-\u9fff]', '', filename_title)[:20].strip('_')

    filename = f"{str(idx+1).zfill(2)}_{filename_title}.csv"
    sub_df.to_csv(os.path.join(output_dir, filename), index=False)
    print(f"âœ… å¯«å…¥ï¼š{filename}")

print(f"\nğŸ‰ å…±åˆ‡å‡º {len(blocks)} æ®µä¸»é¡Œï¼Œå„²å­˜åœ¨ï¼š{output_dir}")