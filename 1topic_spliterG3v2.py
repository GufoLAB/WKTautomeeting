#ä¸€æ¬¡åŒæ™‚çœ‹ä¸€å€‹ç‰‡æ®µä¸¦ä¸”çµ¦å‡ºè¦ºå¾—æ‡‰è©²åˆ†å‰²çš„é»
#é‡è¦åƒæ•¸def batch_cut_by_chunk(lines, lines_per_chunk=40, stride=30, min_gap=6)60 50 6
#python topic_spliterG3v2.py /home/henry/automeeting/20250528æœƒè­°é€šè¦æ ¼/0528æœƒè­°é€šè¦æ ¼_shorten.txt
# åˆ†æ chunk [0:60] å›è¦†ï¼šåˆ‡æ›é»ï¼š3, 4, 10, 19, 31, 35, 40, 55...
# åˆ†æ chunk [50:110] å›è¦†ï¼šåˆ‡æ›é»ï¼š50, 55, 65, 78, 80, 92, 100...
import os
import re
import argparse
import time
from dotenv import load_dotenv
from tqdm import tqdm
from zhconv_rs import zhconv
import ollama
from config import BACK_END_MODEL, AI_MODEL, OLLAMA_URL
from openai import OpenAI
import sys
# ========= INIT =========
# ---------------- core logic åŒ…æˆå‡½å¼ ----------------
def run(input_file: str) -> str:
    starttime = time.time()
    parser = argparse.ArgumentParser(description="ç”¨ GPT å°‡é€å­—ç¨¿ä¾ä¸»é¡Œåˆ†æ®µä¸¦è¼¸å‡º md æª”")
    parser.add_argument("input_file", help="è¼¸å…¥é€å­—ç¨¿æª”æ¡ˆï¼ˆæ¯è¡Œæ ¼å¼ç‚º speakerX: èªªè©±å…§å®¹ï¼‰")
    args = parser.parse_args()
    input_file = args.input_file
    MAX_TEXT_LEN = 3000
    base_name = os.path.splitext(os.path.basename(input_file))[0]  # 0609_shorten
    parent_dir = os.path.dirname(input_file)                       # /home/henry/automeeting/æœƒè­°_20250609171235
    output_dir = os.path.join(parent_dir, base_name + "_topics")   # /home/henry/.../0609_shorten_topics
    os.makedirs(output_dir, exist_ok=True)

    # ========= AI åˆå§‹åŒ– =========
    load_dotenv()
    openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

    def ai_response(conversation_history, max_tokens=1000):
        if BACK_END_MODEL == 'openai':
            response = openai_client.chat.completions.create(
                model=AI_MODEL,
                messages=conversation_history
            )
            assistant_reply = response.choices[0].message.content
        elif BACK_END_MODEL == 'ollama':
            response = ollama.Client(host=OLLAMA_URL).chat(
                model=AI_MODEL,
                messages=conversation_history
            )
            assistant_reply = response['message']['content'].strip()
            if AI_MODEL.startswith("deepseek"):
                assistant_reply = re.sub(r'<think>(.*)</think>', '', assistant_reply, flags=re.DOTALL).strip()
        assistant_reply = zhconv(assistant_reply, "zh-tw")
        return assistant_reply

    # ========= è®€å…¥é€å­—ç¨¿ =========
    with open(input_file, "r", encoding="utf-8") as f:
        lines = [line.strip() for line in f if line.strip()]

    # ========= ç”¨ chunk åˆ†æ®µè«‹ AI åµæ¸¬ä¸»é¡Œåˆ‡é» =========claude
    #def batch_cut_by_chunk(lines, lines_per_chunk=60, stride=50, min_gap=10):
    def batch_cut_by_chunk(lines, lines_per_chunk=60, stride=50, min_gap=10, max_seg_len=60):
        """
        ç”¨ chunk åˆ†æ®µè«‹ AI åµæ¸¬ä¸»é¡Œåˆ‡é»ï¼Œä¸¦ç¢ºä¿æ¯å€‹æ®µè½ä¸è¶…é max_seg_len è¡Œ

        Args:
            lines: é€å­—ç¨¿è¡Œåˆ—è¡¨
            lines_per_chunk: æ¯æ¬¡åˆ†æçš„è¡Œæ•¸
            stride: æ»‘å‹•æ­¥é•·
            min_gap: åˆ‡é»é–“æœ€å°è·é›¢
            max_seg_len: æ¯å€‹æ®µè½æœ€å¤§è¡Œæ•¸

        Returns:
            åˆ‡é»ç´¢å¼•åˆ—è¡¨
        """
        split_indices = [0]
        idx = 0
        total_lines = len(lines)

        # ç¬¬ä¸€éšæ®µï¼šç”¨ AI æ‰¾å‡ºä¸»é¡Œåˆ‡é»
        while idx < total_lines:
            chunk_lines = lines[idx:idx+lines_per_chunk]
            numbered_chunk = [f"[{i+idx}] {line}" for i, line in enumerate(chunk_lines)]
            chunk_text = "\n".join(numbered_chunk)

            prompt = f"""

    ---
    {chunk_text}
    ---
    è«‹é–±è®€ä»¥ä¸Šé€å­—ç¨¿ï¼Œæ‰¾å‡ºæ˜é¡¯çš„ä¸»é¡Œæ®µè½åˆ‡æ›çš„ä½ç½®ã€‚
    è«‹åƒ…è¼¸å‡ºã€Œåˆ‡æ›é»çš„è¡Œè™Ÿã€ï¼ˆå³ä¸­æ‹¬è™Ÿå…§çš„åŸå§‹è¡Œè™Ÿï¼‰ï¼Œæ ¼å¼å¿…é ˆç¬¦åˆï¼š
    åˆ‡æ›é»ï¼š12, 102
    åˆ‡æ›é»ä¹‹é–“è¦æœ‰é©ç•¶çš„è·é›¢ä¿æŒä¸€å€‹é€å­—ç¨¿ä¸­çš„ä¸»é¡Œå®Œæ•´æ€§ï¼
    è«‹åš´æ ¼éµå®ˆï¼Œ**ä¸å‡†è¼¸å‡ºä»»ä½•å…¶ä»–æ–‡å­—èªªæ˜æˆ–æ›è¡Œç¬¦è™Ÿ**ï¼Œå¦å‰‡è¦–ç‚ºéŒ¯èª¤ã€‚

    """

            messages = []
            messages.append({"role": "user", "content": prompt})
            reply = ai_response(messages).strip()
            print(f"ğŸ§ª åˆ†æ chunk [{idx}:{idx+lines_per_chunk}] å›è¦†ï¼š{reply[:50]}...", file=sys.stderr)

            match = re.search(r'åˆ‡æ›é»[:ï¼š]([\d,\s]+)', reply)
            if match:
                nums = match.group(1)
                new_points = [int(n.strip()) for n in nums.split(",") if n.strip().isdigit()]
                split_indices.extend(new_points)

            idx += stride

        # ç¬¬äºŒéšæ®µï¼šéæ¿¾éè¿‘çš„åˆ‡é»
        split_indices = sorted(set(split_indices + [len(lines)]))
        filtered = [split_indices[0]]
        for pt in split_indices[1:]:
            if pt - filtered[-1] >= min_gap:
                filtered.append(pt)

        # ç¬¬ä¸‰éšæ®µï¼šç¢ºä¿æ²’æœ‰æ®µè½è¶…é max_seg_len è¡Œ
        final_indices = [filtered[0]]
        for i in range(1, len(filtered)):
            start = final_indices[-1]
            end = filtered[i]
            seg_len = end - start

            # å¦‚æœæ®µè½å¤ªé•·ï¼Œå¼·åˆ¶æ’å…¥åˆ‡é»
            if seg_len > max_seg_len:
                # å¾ start é–‹å§‹ï¼Œæ¯ max_seg_len è¡Œæ’å…¥ä¸€å€‹åˆ‡é»
                current = start
                while current + max_seg_len < end:
                    current += max_seg_len
                    final_indices.append(current)

            final_indices.append(end)

        return final_indices



    split_indices = batch_cut_by_chunk(lines)
    # ========= åˆ‡æ®µä¸¦åŠ å…¥ padding =========
    padding = 1
    segments = []
    for start_idx, end_idx in zip(split_indices[:-1], split_indices[1:]):
        real_start = max(0, start_idx - padding)
        real_end = end_idx  # âœ… ä¸åŠ å°¾ç«¯ paddingï¼Œé¿å…é‡è¤‡
        segments.append(lines[real_start:real_end])

    # ========= æª”åæ¸…ç† =========
    def sanitize_filename(text):
        text = re.sub(r'[\\/*?:"<>|ï¼ˆï¼‰()ã€ã€‘ã€Œã€ã€ï¼Œã€‚ï¼ï¼Ÿ~`\'\s]+', '_', text)
        text = re.sub(r'[^\w\u4e00-\u9fff]', '', text)
        return text[:20].strip("_")

    # ========= å¯«å…¥ Markdown =========
    total_segments = len(segments)
    for i, segment in enumerate(tqdm(segments, desc="å¯«å…¥ä¸»é¡Œæ®µè½"), 1):
        #text_block = "\n".join(segment)
        text_block = "\n".join(segment[:-1]) if i < total_segments else "\n".join(segment)
        summary_input = text_block[:MAX_TEXT_LEN]
        summary_prompt = f"""è«‹å¯«å‡ºé€™ä¸€æ®µæœ€é‡è¦çš„äº‹æƒ…ï¼Œä¸è¦åŠ æ¨™é»æˆ–è§£é‡‹ï¼š
    ---
    {summary_input}
    ---
    ä¸»é¡Œï¼š"""
        messages = [{"role": "user", "content": summary_prompt}]
        topic_title = ai_response(messages, max_tokens=1000)
        filename_title = sanitize_filename(topic_title)
        filename = f"{i:02d}_{filename_title}.md"
        filepath = os.path.join(output_dir, filename)

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(f"# {topic_title}\n\n")
            f.write(text_block)

        print(f"âœ… å¯«å…¥ï¼š{filename}", file=sys.stderr)

    print(f"\nğŸ‰ å…±åˆ‡å‡º {len(segments)} æ®µä¸»é¡Œï¼Œå„²å­˜åœ¨ï¼š{output_dir}", file=sys.stderr)
    print("time cost", time.time() - starttime, file=sys.stderr)
    return output_dir

# ---------------- CLI å…¥å£ ----------------
def main():
    p = argparse.ArgumentParser(description="ç”¨ GPT å°‡é€å­—ç¨¿ä¾ä¸»é¡Œåˆ†æ®µä¸¦è¼¸å‡º md æª”")
    p.add_argument("input_file", help="è¼¸å…¥é€å­—ç¨¿æª”æ¡ˆï¼ˆæ¯è¡Œæ ¼å¼ç‚º speakerX: å…§å®¹ï¼‰")
    args = p.parse_args()

    out_dir = run(args.input_file)
    print(out_dir)             # â˜… stdout åƒ…å‰©é€™ä¸€è¡Œ

if __name__ == "__main__":
    main()