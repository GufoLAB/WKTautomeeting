import os
import tiktoken
import asyncio
import aiofiles
import time
import re
from dotenv import load_dotenv
from openai import AsyncOpenAI

load_dotenv()
client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# è‡ªè¨‚ä¸»é¡Œæ¸…å–®
cus_topic = [
    "çµ„ç¹”èˆ‡è·è²¬åŠƒåˆ†",
    "å¸‚å ´éŠ·å”®èˆ‡ç›®æ¨™",
    "å¤–éƒ¨ç’°å¢ƒè®Šå‹•èˆ‡é—œç¨…çš„å½±éŸ¿",
    "å…¬å¸æœªä¾†ç™¼å±•æ–¹å‘",
    "äººæ‰åŸ¹é¤Šèˆ‡å­¸ç¿’ç™¼å±•"
]

# ä¸»é¡Œè­˜åˆ¥ prompt
INIT_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€å€‹é€èˆªé–±è®€ä¸¦å°‹æ‰¾ä¸»é¡Œçš„ï¼¡ï¼©
è«‹åš´æ ¼æŒ‰ç…§æ ¼å¼åˆ—å‡ºæœ¬ç¯‡æ–‡æœ¬æ¶‰åŠçš„æ‰€æœ‰ä¸»é¡Œï¼Œä¸é ˆé¡å¤–ä»»ä½•èªªæ˜ï¼š
é€è¡Œä»”ç´°é–±è®€æ–‡æœ¬ï¼Œçœ‹åˆ°æ–°ä¸»é¡Œæ™‚å»ºç«‹ä¸€å€‹æ–°ä¸»é¡Œåç¨±ï¼Œç„¶å¾Œæ ¹æ“šå…¨æ–‡é‡æ–°æ€è€ƒä»¥ä¸Šä¸»é¡Œå“ªäº›æ‡‰è©²åˆä½µä»¥å¾Œæ‰è¼¸å‡ºã€‚
1. ä¸»é¡Œåç¨±
2. ä¸»é¡Œåç¨±
...
"""

def extract_topics(text):
    return re.findall(r"\d+\.\s*(.+)", text)

# GPT relevance åˆ¤æ–· prompt
def make_relevance_prompt(sentence, topic, context=""):
    return f"""
ä½ æ˜¯ä¸€å€‹èªæ„åˆ¤æ–· AIã€‚

è«‹é–±è®€ä»¥ä¸‹ã€å¥å­ã€‘èˆ‡ã€ä¸»é¡Œã€‘ï¼Œåˆ¤æ–·é€™å€‹å¥å­æ˜¯å¦èˆ‡é€™å€‹ä¸»é¡Œé«˜åº¦ç›¸é—œã€‚

- è‹¥ç›¸é—œï¼ˆå¥å­å°è©²ä¸»é¡Œå…·æœ‰ç›´æ¥è²¢ç»æ„ç¾©ï¼‰ï¼Œè«‹å›è¦†ï¼štrue
- è‹¥ç„¡é—œæˆ–åªæ˜¯é–’èŠèƒŒæ™¯è³‡è¨Šï¼Œè«‹å›è¦†ï¼šfalse
- ä¸éœ€è¦æä¾›ä»»ä½•èªªæ˜æˆ–å…¶ä»–æ–‡å­—ï¼Œåªè¦å›è¦† true æˆ– falseã€‚

ä¸»é¡Œï¼š
{topic}

å¥å­ï¼š
{sentence}

ï¼ˆå¯åƒè€ƒä¸Šä¸‹æ–‡è³‡è¨Šå¦‚ä¸‹ï¼Œä½†éå¿…è¦ï¼‰ï¼š
{context}
"""

# GPT éåŒæ­¥å‘¼å«
# åŠ å…¥ token çµ±è¨ˆ
total_prompt_tokens = 0
total_completion_tokens = 0

def count_tokens(text, model="gpt-4o"):
    enc = tiktoken.encoding_for_model(model)
    return len(enc.encode(text))

async def async_call_gpt(prompt, model="gpt-4o"):
    global total_prompt_tokens, total_completion_tokens
    response = await client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}]
    )
    usage = response.usage
    if usage:
        total_prompt_tokens += usage.prompt_tokens
        total_completion_tokens += usage.completion_tokens
    return response.choices[0].message.content.strip().lower()

# åˆ¤æ–·æ˜¯å¦ç›¸é—œ
async def is_relevant_to_topic(sentence, topic, context=""):
    prompt = make_relevance_prompt(sentence, topic, context)
    result = await async_call_gpt(prompt)
    return result == "true"

# åˆ‡å¥å·¥å…·ï¼ˆç°¡å–®å¥é»æ–·å¥ï¼Œå¯å†åŠ å¼·ï¼‰
def split_sentences(text):
    return re.split(r'[\nã€‚ï¼ï¼Ÿ]', text)

# è™•ç†å–®ä¸€ chunk çš„å¥å­éæ¿¾
async def process_chunk(chunk_text, topic, output_dir, idx):
    sentences = split_sentences(chunk_text)
    relevant_sentences = []
    for sent in sentences:
        if sent.strip():
            relevant = await is_relevant_to_topic(sent, topic, context=chunk_text)
            if relevant:
                relevant_sentences.append(sent.strip())

    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"{topic}_chunk_{idx}.txt")
    async with aiofiles.open(output_path, "w", encoding="utf-8") as f:
        await f.write("\n".join(relevant_sentences))
    print(f"âœ… ä¸»é¡Œã€{topic}ã€‘Chunk {idx} ç¯©é¸å®Œæˆï¼Œå…± {len(relevant_sentences)} å¥ç›¸é—œ")

# ä¸»ç¨‹å¼
async def main(input_txt):
    start_time = time.time()

    async with aiofiles.open(input_txt, "r", encoding="utf-8") as f:
        full_text = await f.read()

    # é ä¼° token
    estimated_tokens = count_tokens(full_text)
    print(f"\U0001F4D0 ä¼°ç®—åŸæ–‡ token æ•¸: {estimated_tokens}")

    # GPT ä¸»é¡Œè¾¨è­˜
    init_prompt = INIT_PROMPT_TEMPLATE + "\n\næ–‡æœ¬å…§å®¹:\n" + full_text
    ai_response = await async_call_gpt(init_prompt)
    topics = extract_topics(ai_response)
    topics += cus_topic
    topics = list(set(topics))
    print("\nğŸ¯ ä¸»é¡Œåˆ—è¡¨:", topics)

    # åˆ‡ chunk
    chunks = split_chunks(full_text)

    # è¼¸å‡ºè³‡æ–™å¤¾åç¨±
    output_dir = "topic_sentence_filter"

    # è™•ç†æ¯å€‹ä¸»é¡Œèˆ‡ chunk
    for topic in topics:
        for idx, chunk in enumerate(chunks, 1):
            await process_chunk(chunk, topic, output_dir, idx)

    end_time = time.time()
    print("\nğŸ“Š Token ä½¿ç”¨çµ±è¨ˆï¼š")
    print(f"- Prompt tokens: {total_prompt_tokens}")
    print(f"- Completion tokens: {total_completion_tokens}")
    print(f"- ç¸½ token æ•¸ï¼š{total_prompt_tokens + total_completion_tokens}")
    print(f"\U0001F680 ç¸½åŸ·è¡Œæ™‚é–“ï¼š{end_time - start_time:.2f} ç§’")

# åˆ†æ®µé‚è¼¯
def split_chunks(text, chunk_size=2000, overlap=300):
    chunks, start = [], 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="æ ¹æ“šä¸»é¡Œéæ¿¾ç›¸é—œå¥å­")
    parser.add_argument("input_file", help="è¼¸å…¥çš„ .txt æª”æ¡ˆ")
    args = parser.parse_args()

    asyncio.run(main(args.input_file))