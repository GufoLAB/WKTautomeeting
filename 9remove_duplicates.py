#!/usr/bin/env python3
"""
9remove_duplicates.py - é LLM æ™ºèƒ½å»é‡ç®—æ³•

åŠŸèƒ½ï¼š
1. æ£€æµ‹æŠ¥å‘Šä¸­é‡å¤å‡ºç°çš„æ®µè½ã€æ ‡é¢˜
2. ä½¿ç”¨ç›¸ä¼¼åº¦ç®—æ³•ï¼ˆä¸ä¾èµ– LLMï¼‰
3. æ™ºèƒ½ä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°ï¼Œåˆ é™¤åç»­é‡å¤

ç®—æ³•ï¼š
- ç²¾ç¡®åŒ¹é…ï¼šå®Œå…¨ç›¸åŒçš„è¡Œ
- æ¨¡ç³ŠåŒ¹é…ï¼šä½¿ç”¨ç¼–è¾‘è·ç¦»åˆ¤æ–­ç›¸ä¼¼æ®µè½
- ç»“æ„åˆ†æï¼šè¯†åˆ«é‡å¤çš„ç« èŠ‚ç»“æ„

ç”¨æ³•ï¼š
python 9remove_duplicates.py --input report.md --output report_dedup.md
"""

import argparse
import re
from pathlib import Path
from typing import List, Set, Tuple
from difflib import SequenceMatcher


# ==================== å»é‡ç­–ç•¥ ====================

class DuplicateRemover:
    """æ™ºèƒ½å»é‡å™¨"""

    def __init__(self, similarity_threshold: float = 0.85):
        self.similarity_threshold = similarity_threshold
        self.seen_sections = set()  # å·²è§è¿‡çš„ç« èŠ‚æ ‡é¢˜
        self.seen_lines = set()     # å·²è§è¿‡çš„è¡Œï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
        self.seen_paragraphs = []   # å·²è§è¿‡çš„æ®µè½ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰

        # éœ€è¦å»é‡çš„æ ‡é¢˜æ¨¡å¼ï¼ˆåŒ…å«æœ‰å†’è™Ÿå’Œæ²’å†’è™Ÿçš„ï¼‰
        self.duplicate_headers = [
            r'\*\*æœƒè­°åŸºæœ¬è³‡è¨Š[ï¼š:]*\*\*',
            r'\*\*èˆ‡æœƒäººå“¡[ï¼š:]*\*\*',
            r'\*\*æœƒè­°æ™‚é–“[ï¼š:]*\*\*',
            r'\*\*æœƒè­°åœ°é»[ï¼š:]*\*\*',
            r'\*\*è­°ç¨‹èˆ‡é‡é»æ‘˜è¦[ï¼š:]*\*\*',
            r'\*\*æœƒè­°ç›®çš„[ï¼š:]*\*\*',
        ]

    def similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤æ®µæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰"""
        return SequenceMatcher(None, text1, text2).ratio()

    def is_duplicate_header(self, line: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯é‡å¤çš„æ ‡é¢˜"""
        for pattern in self.duplicate_headers:
            if re.search(pattern, line):
                return True
        return False

    def is_empty_or_useless(self, line: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯ç©ºè¡Œæˆ–æ— ç”¨è¡Œ"""
        stripped = line.strip()

        # ç©ºè¡Œ
        if not stripped:
            return False  # ä¿ç•™ç©ºè¡Œç”¨äºæ ¼å¼

        # åªæœ‰æ˜Ÿå·çš„è¡Œ
        if re.match(r'^[\*\s]+$', stripped):
            return True

        # ç©ºçš„åˆ—è¡¨é¡¹
        if re.match(r'^\*\s*$', stripped):
            return True

        # å¤šå±‚ç©ºåˆ—è¡¨
        if re.match(r'^\*\s+\*\s+\*\s+\*\s*$', stripped):
            return True

        return False

    def normalize_line(self, line: str) -> str:
        """æ ‡å‡†åŒ–è¡Œï¼ˆç”¨äºæ¯”è¾ƒï¼‰"""
        # å»é™¤å¤šä½™ç©ºæ ¼
        normalized = ' '.join(line.split())
        # å»é™¤æ ‡ç‚¹ç¬¦å·çš„ç©ºæ ¼å·®å¼‚
        normalized = re.sub(r'\s*([ï¼š:ï¼Œã€‚])\s*', r'\1', normalized)
        return normalized

    def is_similar_to_seen(self, paragraph: str) -> bool:
        """æ£€æŸ¥æ®µè½æ˜¯å¦ä¸å·²è§è¿‡çš„ç›¸ä¼¼"""
        normalized = self.normalize_line(paragraph)

        for seen_para in self.seen_paragraphs:
            similarity = self.similarity(normalized, seen_para)
            if similarity >= self.similarity_threshold:
                return True

        return False

    def process_content(self, content: str) -> str:
        """å¤„ç†å†…å®¹ï¼Œå»é™¤é‡å¤"""
        lines = content.split('\n')
        result = []
        current_paragraph = []
        in_topic_section = False
        topic_section_index = 0
        in_header_section = True  # é–‹é ­å€åŸŸæ¨™è¨˜

        for i, line in enumerate(lines):
            stripped = line.strip()

            # æ£€æµ‹ä¸»é¢˜åˆ†éš”ï¼ˆ## æ•°å­—.ï¼‰
            if re.match(r'^##\s+\d+\.', stripped):
                in_topic_section = True
                in_header_section = False  # é›¢é–‹é–‹é ­å€åŸŸ
                topic_section_index += 1

                result.append(line)
                continue

            # æ£€æµ‹ä¼šè®®æ‘˜è¦éƒ¨åˆ†ç»“æŸ
            if stripped.startswith('## ') and 'æœƒè­°æ‘˜è¦' in stripped:
                in_header_section = False

            # åœ¨ä¸»é¢˜å†…å®¹ä¸­æ£€æµ‹é‡å¤æ ‡é¢˜ - å…¨éƒ¨åˆªé™¤ï¼Œä¸ç®¡æ˜¯å¦å‡ºç¾é
            if in_topic_section and self.is_duplicate_header(line):
                # ç›´æ¥è·³éï¼Œä¸ä¿ç•™ä»»ä½•ä¸€æ¬¡
                continue

            # æ£€æŸ¥æ— ç”¨è¡Œ
            if self.is_empty_or_useless(line):
                # ä¿ç•™ä¸€å®šæ•°é‡çš„ç©ºè¡Œç”¨äºæ ¼å¼
                if stripped == '' and result and result[-1].strip() != '':
                    result.append(line)
                continue

            # æ®µè½çº§å»é‡ï¼ˆç”¨äºè¾ƒé•¿çš„é‡å¤å†…å®¹ï¼‰
            if stripped and len(stripped) > 50:
                if self.is_similar_to_seen(stripped):
                    continue
                else:
                    # åªä¿ç•™å‰ 100 ä¸ªæ®µè½ç”¨äºæ¯”è¾ƒï¼ˆé¿å…å†…å­˜è¿‡å¤§ï¼‰
                    if len(self.seen_paragraphs) < 100:
                        self.seen_paragraphs.append(self.normalize_line(stripped))

            result.append(line)

        # æ¸…ç†è¿ç»­çš„å¤šä¸ªç©ºè¡Œ
        cleaned = []
        prev_empty = False
        for line in result:
            is_empty = line.strip() == ''
            if is_empty and prev_empty:
                continue
            cleaned.append(line)
            prev_empty = is_empty

        return '\n'.join(cleaned)


# ==================== ç»Ÿè®¡åˆ†æ ====================

def analyze_duplicates(content: str) -> dict:
    """åˆ†æé‡å¤æƒ…å†µï¼ˆä¸ä¿®æ”¹å†…å®¹ï¼‰"""
    lines = content.split('\n')

    # ç»Ÿè®¡æ ‡é¢˜å‡ºç°æ¬¡æ•°
    header_counts = {}
    for line in lines:
        stripped = line.strip()
        if re.match(r'\*\*[^*]+\*\*', stripped):
            header_counts[stripped] = header_counts.get(stripped, 0) + 1

    # æ‰¾å‡ºé‡å¤çš„æ ‡é¢˜
    duplicates = {k: v for k, v in header_counts.items() if v > 1}

    # ç»Ÿè®¡ç©ºçš„åˆ—è¡¨é¡¹
    empty_list_items = sum(1 for line in lines if re.match(r'^\s*\*\s*\*\s*\*\s*\*\s*$', line.strip()))

    return {
        'total_lines': len(lines),
        'duplicate_headers': duplicates,
        'empty_list_items': empty_list_items
    }


# ==================== ä¸»ç¨‹åº ====================

def main():
    parser = argparse.ArgumentParser(
        description='æ™ºèƒ½å»é‡æŠ¥å‘Šå†…å®¹ï¼ˆé LLMï¼‰'
    )

    parser.add_argument('--input', required=True, help='è¾“å…¥æ–‡ä»¶')
    parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶ï¼ˆé»˜è®¤ï¼šinput_dedup.mdï¼‰')
    parser.add_argument('--similarity', type=float, default=0.85,
                       help='ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼Œé»˜è®¤ 0.85ï¼‰')
    parser.add_argument('--analyze-only', action='store_true',
                       help='åªåˆ†æä¸ä¿®æ”¹')

    args = parser.parse_args()

    input_file = Path(args.input)
    if not input_file.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼š{input_file}")
        return

    # è¯»å–å†…å®¹
    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read()

    print("="*60)
    print("ğŸ” æ™ºèƒ½å»é‡åˆ†æ")
    print("="*60)

    # åˆ†æé‡å¤
    stats = analyze_duplicates(content)
    print(f"\nğŸ“Š åŸå§‹çµ±è¨ˆï¼š")
    print(f"  - ç¸½è¡Œæ•¸ï¼š{stats['total_lines']}")
    print(f"  - é‡è¤‡æ¨™é¡Œï¼š{len(stats['duplicate_headers'])} ç¨®")

    if stats['duplicate_headers']:
        print(f"\nğŸ” ç™¼ç¾é‡è¤‡æ¨™é¡Œï¼š")
        for header, count in sorted(stats['duplicate_headers'].items(),
                                    key=lambda x: x[1], reverse=True):
            if count > 2:  # åªæ˜¾ç¤ºé‡å¤ 2 æ¬¡ä»¥ä¸Šçš„
                print(f"  - {header[:50]}... (å‡ºç¾ {count} æ¬¡)")

    print(f"  - ç©ºåˆ—è¡¨é …ï¼š{stats['empty_list_items']}")

    if args.analyze_only:
        print("\nâœ… åˆ†æå®Œæˆï¼ˆæœªä¿®æ”¹æ–‡ä»¶ï¼‰")
        return

    # æ‰§è¡Œå»é‡
    print(f"\nğŸ”„ åŸ·è¡Œå»é‡...")
    print(f"  - ç›¸ä¼¼åº¦é–¾å€¼ï¼š{args.similarity}")

    remover = DuplicateRemover(similarity_threshold=args.similarity)
    cleaned_content = remover.process_content(content)

    # ç»Ÿè®¡å»é‡å
    stats_after = analyze_duplicates(cleaned_content)

    # ç¡®å®šè¾“å‡ºè·¯å¾„
    if args.output:
        output_file = Path(args.output)
    else:
        output_file = input_file.parent / f"{input_file.stem}_dedup.md"

    # ä¿å­˜
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(cleaned_content)

    print(f"\nâœ… å»é‡å®Œæˆï¼")
    print(f"\nğŸ“Š å»é‡å¾Œçµ±è¨ˆï¼š")
    print(f"  - ç¸½è¡Œæ•¸ï¼š{stats_after['total_lines']} (æ¸›å°‘ {stats['total_lines'] - stats_after['total_lines']} è¡Œ)")
    print(f"  - é‡è¤‡æ¨™é¡Œï¼š{len(stats_after['duplicate_headers'])} ç¨® (æ¸›å°‘ {len(stats['duplicate_headers']) - len(stats_after['duplicate_headers'])} ç¨®)")

    # è®¡ç®—å‹ç¼©ç‡
    original_size = len(content)
    cleaned_size = len(cleaned_content)
    reduction = (1 - cleaned_size / original_size) * 100

    print(f"  - æª”æ¡ˆå¤§å°ï¼š{cleaned_size / 1024:.1f} KB (æ¸›å°‘ {reduction:.1f}%)")
    print(f"\nğŸ’¾ å·²å„²å­˜ï¼š{output_file}")

    print("\n" + "="*60)
    print("ğŸ‰ å®Œæˆï¼")
    print("="*60)


if __name__ == '__main__':
    main()
